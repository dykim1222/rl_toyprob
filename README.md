# Toy Problem for Non-sampling Reinforcement Learning

* Dynamic programming (DP) is not applicable since we are not allowed to sample the future.
* It's not obvious to derive a parallel theorem to Bellman equations as in DP or reinforcement learning (RL).
* To visualize histogram for rewards for policies. 
* To study behavior as horizon T increases and when theta is deterministic.
* Visualize the transition between exploration & exploitation: might lead us the way how to implement/quantify exp-exp; further turning this into an objective function.
